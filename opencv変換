import pyds
import numpy as np
import cv2

def display_frame(buffer, batch_meta):
    # フレームごとに処理
    l_frame = batch_meta.frame_meta_list
    while l_frame is not None:
        try:
            frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data)
        except StopIteration:
            break

        # フレーム番号とバッチID取得
        frame_index = frame_meta.batch_id
        width = frame_meta.source_frame_width
        height = frame_meta.source_frame_height

        # NvBufSurface を取得
        surface = pyds.get_nvds_buf_surface(hash(buffer), frame_index)

        # GPUメモリからCPUメモリへコピー（RGBA）
        frame_rgba = np.array(surface, copy=True, order='C')  # shape=(H, W, 4)

        # RGBA → BGR（OpenCV形式）
        frame_bgr = cv2.cvtColor(frame_rgba, cv2.COLOR_RGBA2BGR)

        # OpenCVで表示
        cv2.imshow("DeepStream Frame", frame_bgr)
        cv2.waitKey(1)

        try:
            l_frame = l_frame.next
        except StopIteration:
            break








import gi
gi.require_version('Gst', '1.0')
from gi.repository import Gst, GObject

Gst.init(None)

# パイプライン作成
pipeline = Gst.Pipeline()

# 映像入力、処理、muxerなどの設定
source = Gst.ElementFactory.make("uridecodebin", "source")
streammux = Gst.ElementFactory.make("nvstreammux", "streammux")
# ...（他の要素：nvinfer, nvvidconv, nvdsosdなど）

# 分岐用のtee
tee = Gst.ElementFactory.make("tee", "tee")

# sink1: ディスプレイ出力
queue_display = Gst.ElementFactory.make("queue", "queue_display")
sink_display = Gst.ElementFactory.make("nveglglessink", "eglsink")

# sink2: ファイル出力（エンコード付き）
queue_file = Gst.ElementFactory.make("queue", "queue_file")
encoder = Gst.ElementFactory.make("nvv4l2h264enc", "encoder")
parser = Gst.ElementFactory.make("h264parse", "parser")
muxer = Gst.ElementFactory.make("qtmux", "muxer")  # mp4出力用
sink_file = Gst.ElementFactory.make("filesink", "filesink")
sink_file.set_property("location", "/path/to/output.mp4")

# パイプラインに追加し、リンク
for elem in [source, streammux, tee, queue_display, sink_display, queue_file, encoder, parser, muxer, sink_file]:
    pipeline.add(elem)

# teeから両方向に分岐
tee.link(queue_display)
queue_display.link(sink_display)

tee.link(queue_file)
queue_file.link(encoder)
encoder.link(parser)
parser.link(muxer)
muxer.link(sink_file)






# 分岐点
tee = Gst.ElementFactory.make("tee", "tee")

# display出力パス
queue_display = Gst.ElementFactory.make("queue", "queue_display")
sink_display = Gst.ElementFactory.make("nveglglessink", "sink_display")
sink_display.set_property("sync", False)

# file出力パス
queue_file = Gst.ElementFactory.make("queue", "queue_file")
encoder = Gst.ElementFactory.make("nvv4l2h264enc", "encoder")
parser = Gst.ElementFactory.make("h264parse", "parser")
muxer = Gst.ElementFactory.make("qtmux", "muxer")
sink_file = Gst.ElementFactory.make("filesink", "sink_file")
sink_file.set_property("location", "/tmp/output.mp4")
sink_file.set_property("sync", False)

# パイプラインに追加
for elem in [tee, queue_display, sink_display, queue_file, encoder, parser, muxer, sink_file]:
    pipeline.add(elem)

# 分岐リンク
tee.link(queue_display)
queue_display.link(sink_display)

tee.link(queue_file)
queue_file.link(encoder)
encoder.link(parser)
parser.link(muxer)
muxer.link(sink_file)







import pyds
import numpy as np
import ctypes
import cv2

def convert_nvds_buf_to_cv_image(gst_buffer, batch_id):
    """
    DeepStreamのGPUバッファからOpenCVで扱えるNumPy画像を取得する
    - gst_buffer: Gst.Buffer
    - batch_id: このバッファ内のフレーム番号
    """
    # Get NvBufSurface from gst_buffer
    surface = pyds.get_nvds_buf_surface(hash(gst_buffer), batch_id)

    # Deep copy to CPU for OpenCV
    img_array = np.array(surface, copy=True, order='C')

    # OpenCVはBGRを使う
    img = cv2.cvtColor(img_array, cv2.COLOR_RGBA2BGR)

    return img


def osd_sink_pad_buffer_probe(pad, info, u_data):
    gst_buffer = info.get_buffer()
    if not gst_buffer:
        return Gst.PadProbeReturn.OK

    # バッチメタを取得
    batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))

    l_frame = batch_meta.frame_meta_list
    while l_frame is not None:
        try:
            frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data)
        except StopIteration:
            break

        frame_width = int(frame_meta.source_frame_width)
        frame_height = int(frame_meta.source_frame_height)

        # バッファからOpenCV画像を取得（RGB/BGR）
        # ※別途cv_image変換処理が必要（例: nvbufsurface → numpy）
        frame_image = convert_nvds_buf_to_cv_image(gst_buffer, frame_meta.batch_id)

        l_obj = frame_meta.obj_meta_list
        while l_obj is not None:
            try:
                obj_meta = pyds.NvDsObjectMeta.cast(l_obj.data)
            except StopIteration:
                break

            # マスク描画
            frame_image = draw_instance_mask(frame_image, obj_meta)

            try:
                l_obj = l_obj.next
            except StopIteration:
                break

        # 必要なら frame_image を保存や表示処理へ渡す

        try:
            l_frame = l_frame.next
        except StopIteration:
            break

    return Gst.PadProbeReturn.OK





obj_meta = pyds.NvDsObjectMeta.cast(l_obj.data)
if obj_meta.mask_params:
    mask = obj_meta.mask_params
    width = mask.width
    height = mask.height
    data = pyds.get_mask_array(mask)  # numpy配列として取得可能

dataはNumPy配列（uint8）で、各ピクセルに1（前景）または0（背景）が含まれます。

このデータを使えば、マスクの「塗られている部分」の相対的な座標を取り出せます。

# マスクのnumpy配列から前景ピクセルの座標を取得
ys, xs = np.where(mask_np == 1)

# 物体のバウンディングボックス
x_offset = obj_meta.rect_params.left
y_offset = obj_meta.rect_params.top
scale_x = obj_meta.rect_params.width / mask.width
scale_y = obj_meta.rect_params.height / mask.height

# マスク座標を元画像座標へ変換
xs_abs = xs * scale_x + x_offset
ys_abs = ys * scale_y + y_offset








. Python バインディング（pyds）でマスク取得
Pythonアプリケーションでも pyds.NvDsObjectMeta.mask_params にアクセスできます。
ただし、6.3の pyds には get_segmentation_masks() というユーティリティ関数はデフォルトでは含まれていない可能性があります。

回避策（マスクデータ取得）
python
コードをコピーする
import pyds
import numpy as np
import ctypes

def get_mask_array(mask_params):
    width = mask_params.width
    height = mask_params.height
    size = int(width * height)

    # ctypes で pointer を扱う
    mask_ptr = ctypes.cast(mask_params.data, ctypes.POINTER(ctypes.c_uint8))
    mask_array = np.ctypeslib.as_array(mask_ptr, shape=(size,))
    mask_array = mask_array.reshape((height, width))

    return mask_array
3. OpenCVで輪郭取得
上記 mask_array を OpenCV に渡して座標リストを取得できます。

python
コードをコピーする
import cv2

mask = get_mask_array(obj.mask_params)
contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

for contour in contours:
    print("Contour coordinates:", contour[:, 0, :])  # (x, y) 座標
4. 注意点
マスクはオブジェクトのバウンディングボックス内に収まっていることが多く、マスクの座標をフレーム全体の座標に変換する必要があります。

obj.rect_params.left/top を使ってオフセットすることで変換可能です。

python
コードをコピーする
x_offset = int(obj.rect_params.left)
y_offset = int(obj.rect_params.top)
contour_global = contour + np.array([[x_offset, y_offset]])
5. 関連 DeepStream 設定ファイル
config_infer_primary_segnet.txt の network-type=3 （セグメンテーション）を指定

output-tensor-meta=1 を忘れずに設定（Python APIで推論出力を取得するため）

結論
DeepStream 6.3 でも、Python + OpenCV でセグメンテーションマスクの座標（輪郭やピクセル位置）は取得可能です。mask_params.data を Numpy に変換する処理だけ注意が必要です。

ご希望であれば、DeepStreamの設定ファイル例やサンプルコード全体も提供可能です。どんなモデルを使っているか教えてもらえれば、もっと具体的にサポートできます。









ChatGPT の回答は必ずしも正しいとは限りません。重要な情報は確認するように
